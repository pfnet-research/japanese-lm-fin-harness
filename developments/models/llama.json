[
    {
        "model": "meta-llama/Llama-2-7b-hf",
        "model_args": [
            "dtype=float16"
        ],
        "run_type": "vllm",
        "memory_Gi": 32,
        "n_gpu": 1,
        "gpu_request": "gpu-any-24gb",
        "a100-80gb": 1,
        "a30-24gb": 1,
        "v100-32gb": 1,
        "v100-16gb": 0,
        "require_hf_login": true
    },
    {
        "model": "meta-llama/Llama-2-7b-chat-hf",
        "model_args": [
            "dtype=float16"
        ],
        "run_type": "vllm",
        "memory_Gi": 32,
        "n_gpu": 1,
        "gpu_request": "gpu-any-24gb",
        "a100-80gb": 1,
        "a30-24gb": 1,
        "v100-32gb": 1,
        "v100-16gb": 0,
        "require_hf_login": true
    },
    {
        "model": "meta-llama/Llama-2-13b-hf",
        "model_args": [
            "dtype=float16"
        ],
        "run_type": "vllm",
        "memory_Gi": 64,
        "n_gpu": 4,
        "gpu_request": "gpu-any-32gb",
        "a100-80gb": 1,
        "a30-24gb": 0,
        "v100-32gb": 4,
        "v100-16gb": 0,
        "require_hf_login": true
    },
    {
        "model": "meta-llama/Llama-2-13b-chat-hf",
        "model_args": [
            "dtype=float16"
        ],
        "run_type": "vllm",
        "memory_Gi": 64,
        "n_gpu": 4,
        "gpu_request": "gpu-any-32gb",
        "a100-80gb": 1,
        "a30-24gb": 0,
        "v100-32gb": 4,
        "v100-16gb": 0,
        "require_hf_login": true
    },
    {
        "model": "meta-llama/Llama-2-70b-hf",
        "model_args": [
            "dtype=float16",
            "tensor_parallel_size=2"
        ],
        "run_type": "vllm",
        "memory_Gi": 128,
        "n_gpu": 2,
        "gpu_request": "gpu-a100",
        "a100-80gb": 2,
        "a30-24gb": 0,
        "v100-32gb": 0,
        "v100-16gb": 0,
        "require_hf_login": true
    },
    {
        "model": "meta-llama/Llama-2-70b-chat-hf",
        "model_args": [
            "dtype=float16",
            "tensor_parallel_size=2"
        ],
        "run_type": "vllm",
        "memory_Gi": 128,
        "n_gpu": 2,
        "gpu_request": "gpu-a100",
        "a100-80gb": 2,
        "a30-24gb": 0,
        "v100-32gb": 0,
        "v100-16gb": 0,
        "require_hf_login": true
    },
    {
        "model": "meta-llama/Meta-Llama-3-8B",
        "model_args": [
            "dtype=bfloat16"
        ],
        "run_type": "vllm",
        "memory_Gi": 64,
        "n_gpu": 1,
        "gpu_request": "gpu-a30",
        "a100-80gb": 1,
        "a30-24gb": 1,
        "v100-32gb": 0,
        "v100-16gb": 0,
        "require_hf_login": true
    },
    {
        "model": "meta-llama/Meta-Llama-3-8B-Instruct",
        "model_args": [
            "dtype=bfloat16"
        ],
        "run_type": "vllm",
        "memory_Gi": 64,
        "n_gpu": 1,
        "gpu_request": "gpu-a30",
        "a100-80gb": 1,
        "a30-24gb": 1,
        "v100-32gb": 0,
        "v100-16gb": 0,
        "require_hf_login": true
    },
    {
        "model": "meta-llama/Meta-Llama-3-70B",
        "model_args": [
            "dtype=bfloat16",
            "tensor_parallel_size=2"
        ],
        "run_type": "vllm",
        "memory_Gi": 128,
        "n_gpu": 2,
        "gpu_request": "gpu-a100",
        "a100-80gb": 2,
        "a30-24gb": 0,
        "v100-32gb": 0,
        "v100-16gb": 0,
        "require_hf_login": true
    },
    {
        "model": "meta-llama/Meta-Llama-3-70B-Instruct",
        "model_args": [
            "dtype=bfloat16",
            "tensor_parallel_size=2"
        ],
        "run_type": "vllm",
        "memory_Gi": 128,
        "n_gpu": 2,
        "gpu_request": "gpu-a100",
        "a100-80gb": 2,
        "a30-24gb": 0,
        "v100-32gb": 0,
        "v100-16gb": 0,
        "require_hf_login": true
    }
]